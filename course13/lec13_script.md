大家好，欢迎来到由IDEA研究院基础软件中心为大家带来的现代编程思想公开课。今天介绍的主题是利用上一节课介绍的自动微分，从零开始构造一个基于梯度下降的神经网络。

首先来简单介绍一下今天我们的案例。我们今天的案例是鸢尾花数据集。这个数据集可以算是机器学习里的Hello World。它公布于1936年，包含了对三种鸢尾花的测量数据，每一种各50个样本，每个样本都包括花萼的长度和宽度，以及花瓣的长度和宽度。我们今天的目标是利用鸢尾花数据集，根据给定的花萼的长度和宽度以及花瓣的长度和宽度，推断出它属于哪一类鸢尾花。这是一个简单的分类的任务，我们希望可以学习到95%以上的正确率。
通常情况下，在进行机器学习前，一个重要的任务便是了解数据集。这里我们的重点在于神经网络的实现，因此略过。
机器学习博大精深，神经网络相关的知识更是繁复，因此，我们这里只是一个简单的介绍。同时，由于代码量较大，今天的视频中只展示部分的样例代码。我们的公开课的完整代码以及学习资料会放在我们的试用平台，也就是try.moonbitlang.cn上，供大家参考。 也欢迎大家从零开始，自己动手，尝试实现。需要注意的是，我们会利用到上节课所介绍的自动微分，因此如果没有看过上一节课的，可以去参考一下。

我们今天的课程大致分为两个部分，构建神经网络，以及训练神经网络。首先来介绍一下神经网络。神经网络属于机器学习的一种。顾名思义，神经网络模拟了人的大脑神经结构。它是一个神经元构成的网络。在右侧，可以看到一个神经网络的示意图。其中，每一个圆对应了一个神经元。一个神经元通常有多个输入，一个输出。例如，这里的黄色的神经元就从左侧的四个神经元接受输入，再将输出传递给右侧的神经元。在我们的例子中，神经信号便是一个浮点数，而每一个神经元都有一些权重来根据输入信号计算出输出信号。一个神经元通常在达到一定的阈值以后激活。一个经典的神经网络通常分为多层结构。

一个经典的神经网络通常包含输入层，输出层，以及处于之间的一层或多层隐含层。输入层用于接受输入的参数，输出层用于给出计算的结果。而对于隐含层而言，如果层数增多，那么我们就算进入了深度学习的领域。深度，便指的是这里的层次的深浅。 神经网络的架构便包含了隐含层的层数、节点数、以及节点的连接方式等。例如，前馈神经网络中，神经元的连接不会形成环路。神经元的激活函数等也是神经网络的超参数之一。这里每一项参数的选择展开来讲都可以是一个复杂的课题，因此今天仅作简单的展示。

我们今天设计的神经网络，如右图所示。输入层有四个节点，这是因为每一个样本都包含四个特征：花瓣和花萼的长宽。输出层有三个节点，分别给出属于每一种鸢尾花的可能性，那么具体的结论就取决于哪一个可能性最大。网络架构我们选择最简单的全连接的前馈神经网络，也就是每一个神经元都与前一层的所有神经元相连。

对于每一个神经元，它的输出都是基于输入的线性变换：f = w0 x0 + w1 x1 + ... + wn xn + c，其中wi和c都是参数，而xi则是输入。对于激活函数，在隐含层，我们选择简单的线性整流函数：当计算值小于零，神经元不激活，因此输出信号为零；而当计算值大于零，则为该计算值。这也符合我们对大脑神经元的简单的模拟。而在输出层，我们则是选择使用Softmax函数。它能够将多个输出整理为总和为1的概率分布。公式十分简单，首先对每一个输出计算出自然常数的幂。如此一来，由于指数函数增长迅速，概率之间的差距便被拉大。之后，求出它们的和，而每一个输出便是对应的幂除以总和。这样，就能保证输出的和是1。

到这里为止，今天用的神经网络的结构就大致介绍完了。让我们稍微来看一看实现。首先我们定义一个运算的抽象。我们所要进行的运算，包含了从浮点数转换到某一类型、从该类型取出当前的中间结果、以及加乘除、取负数，和指数运算。符合这一运算的类型有浮点数本身，以及我们用到的自动微分的类型。具体的可以参考前一个视频。

之后我们开始实现我们的神经网络。首先是激活函数。线性整流函数的实现十分简单，对当前计算值进行一次正负的判断即可。而对于softmax函数，我们首先构造一个输出的数组，默认值都是0。之后，我们利用一个for循环遍历所有的输入值，对它们的指数求和，并再次遍历，求出对应的概率并修改输出的数组。最后，我们返回这个输出的数组。

接下来是从输入层到隐含层的函数。注意这里的输入均为浮点数，而参数则为参与运算的抽象的类型。我们同样先准备一个默认值均为零的数组作为输出。我们遍历每一个节点，并同时遍历每一个输入，形成一个嵌套的双层循环。于是，我们累加输入与参数的乘积，如第5行所示。最后在第七行，我们不能漏了线性变换的常数，并且我们将结果利用管道运算符，输入线性整流函数中，获得最终的输出。最后，我们把输出的数组作为结果返回。

从隐含层到输出层与之前类似，唯一的区别可能就在于我们并没有利用线性整流函数，而是在所有的输出计算完成之后将它整个放入softmax中。到这一步，构建神经网络的关键因素便介绍完了。接下来的问题是，具体的参数该是多少呢？

事实上，没有人能说出参数应该是多少，而是需要通过训练来得到。而如果要评估训练的好坏，我们需要一个损失函数。损失函数能告诉我们当前结果与预期结果之间的差距。在这里，由于我们是进行一个多分类的问题，我们采用交叉熵作为损失函数。而有了损失函数，我们知道了差距，那我们就需要知道，该往哪个方向调整，以及调整多少。在这里我们选择的是梯度下降，也就是对每一个参数求出偏微分。这个梯度决定了参数的调整方向。而调整多少则取决于学习率。我们在这里选择指数下降。接下来逐一介绍并实现。

首先是损失函数。世界上没有一个统一的损失函数，因为每一个机器学习处理的目标不同。我们处理的是一个多分类问题，也就是需要知道输入属于哪一个分类，因此我们在这里选择了多分类问题交叉熵。它的计算是根据对应事件，或者在我们这里，分类，求出的概率取对数后取负数。我们可以这么理解，对于一个分类，理想的结果应该是百分之百，而我们计算出的结果则是当前值，它们之间的差值就是当前结果与预期结果之间的差距。而对数函数取负数则放大了这个差距，差距越是大，实际值越接近零，那么损失函数的值就会越大。

有了损失函数，那么下一步就是后向微分。这里由于上节课有介绍，这里就简单展示代码。我们从二阶数组中构造参数，此处需要两个数组：value提供值，而diff累加偏微分。

之后我们则根据计算出的偏微分，以及step，也就是学习率，对原来的参数进行调整。

然后我们则是需要调整学习率。不合适的学习率可能导致学习成果的下降，甚至永远无法收敛获得最佳结果。图中只是一个概念上的示例，并不对应我们的具体场景。如果我们想要寻找函数的最低点，也就是损失函数最小的地方，我们不断通过求导、移动相应步长来逼近。但是如果步长恒定不变，那么我们就会冲过最低点，然后再返回，再次冲过最低点，始终在下方震荡，无法收敛。因此，我们在这里选择了指数衰减学习率。a和b为常数，x为迭代次数。也就是说，学习率会随着训练次数的增加而逐渐减小，以达到在初始阶段快速逼近，在最后阶段逐渐逼近的效果。

最后的最后，便是利用数据训练我们的神经网络。我们需要将数据随机地分为两个部分：训练集和验证集。在训练阶段，我们在每一轮根据训练集的数据进行计算、求出损失函数并进行微分，并在最后根据学习率调整参数。而验证集则是在所有的训练完成之后验证成果。这是为了避免过拟合，对训练集掌握过好而对一般的情形出错。这也是为什么训练集的数据需要随机拆分。举个例子来说，我们的完整数据集包含三种鸢尾花。如果训练集只包含两种鸢尾花，那么它永远也无法正确判断出第三种鸢尾花。我们的训练量较少，因此可以对所有的数据进行完整训练，也就是每一轮训练都用训练集中的所有数据。而如果数据较多，那么需要考虑分批训练，即每次抽取随机的一部分。

到这里，所有的关键元素便介绍完了。本章节介绍了神经网络的基础知识，包含结构以及训练。具体的实现大家可以参考月兔的试用环境。过往的所有课程讲义也都可以找到。感兴趣的同学也可以尝试挑战其他的数据集。有任何反馈欢迎在评论区留言或联系MoonBit小助手。感谢大家的观看，我们下次再见。
